{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines how the training data is obtained. It requires the python modules located in this folder.\n",
    "\n",
    "### Weather Data\n",
    "\n",
    "There are several points where intermediate data frames are written to disk in order to provide incremental checkpoints, since this dataset takes a while to download and build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weather_data import get_station_df\n",
    "from os.path import isfile\n",
    "from datetime import timedelta, timezone, datetime\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "parallel = Parallel(n_jobs=num_cores)\n",
    "\n",
    "stations_dict = {'IA' : ['DSM', 'CID'],\n",
    "                 'MN' : ['DLH', 'JKJ', 'LYV', 'MSP', 'RST'], \n",
    "                 'WI' : ['MSN', 'MKE', 'EAU', 'GRB'],\n",
    "                 'MI' : ['ANJ', 'GRR', 'LAN', 'DET', 'ARB'],\n",
    "                 'IN' : ['EVV', 'FWA', 'GYY', 'IND', 'SBN', 'SPI'],\n",
    "                 'IL' : ['BMI', 'CMI', 'ARR', 'PIA'],\n",
    "                 'MO' : ['STL', 'COU', 'SGF', 'MKC'],\n",
    "                 'MS' : ['HKS', 'MJD', 'TUP', 'MEI'],\n",
    "                 'LA' : ['BTR', 'LFT', 'LCH', 'SHV', 'AEX'],\n",
    "                 'TX' : ['LFK'] }\n",
    "\n",
    "stations = [(state, station) for state in stations_dict.keys() for station in stations_dict[state]]\n",
    "\n",
    "def download_file_path(state, station):\n",
    "    return f'../data/{state}_{station}.parquet'\n",
    "\n",
    "def est(yyyy, mm, dd, hh):\n",
    "    return datetime(yyyy, mm, dd, hh, tzinfo=timezone(timedelta(hours=-5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_hour = est(2015,  2, 1,  0)\n",
    "last_hour  = est(2021, 12, 31, 23)\n",
    "observation_dates = pd.date_range(start = first_hour, end = last_hour)\n",
    "observation_hours = [d.replace(hour = h) for d in observation_dates for h in range(0, 24)]\n",
    "\n",
    "def normalize_station(station):\n",
    "    station['valid'] = pd.to_datetime(station['valid'], utc = True)\n",
    "    station = station[station['tmpf'] != 'M']\n",
    "    numeric_cols = ['tmpf', 'lat', 'lon']\n",
    "    station[numeric_cols] = station[numeric_cols].apply(pd.to_numeric, axis=1)\n",
    "\n",
    "    station['feel'] = np.where(station['feel'] == 'M', station['tmpf'], station['feel'])\n",
    "    station['feel'] = station[['feel']].apply(pd.to_numeric, axis=1)\n",
    "    return station\n",
    "\n",
    "def download_station(state, station):\n",
    "    path = download_file_path(state, station) \n",
    "    if isfile(path):\n",
    "        return pd.read_parquet(path)\n",
    "\n",
    "    station = get_station_df(station, first_hour, last_hour)\n",
    "    if station is None:\n",
    "        print(f'Retrieve {station} failed')\n",
    "        return None\n",
    "    return station.to_parquet(path)\n",
    "\n",
    "_ = parallel(delayed(download_station)(state, station) for (state, station) in stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read DSM\n",
      "Hourly subsampled DSM\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['valid'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (state, station) \u001b[38;5;129;01min\u001b[39;00m stations:\n\u001b[0;32m---> 26\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbuild_hourly_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m merged \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m dfs[\u001b[38;5;241m1\u001b[39m:]:\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mbuild_hourly_df\u001b[0;34m(state, station)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourly subsampled \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Each station's data will be merged into a single dataframe, so rename columns\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtmpf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m w \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmpf\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_tmpf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeel\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_feel\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#w = normalize_station(w)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3464\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3463\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3464\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3466\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1314\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 1314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_read_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_i8_conversion(ax\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1317\u001b[0m     ax, (IntervalIndex, CategoricalIndex)\n\u001b[1;32m   1318\u001b[0m ):\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# For CategoricalIndex take instead of reindex to preserve dtype.\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;66;03m#  For IntervalIndex this is to map integers to the Intervals they match to.\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1376\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 1377\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['valid'] not in index\""
     ]
    }
   ],
   "source": [
    "def build_hourly_df(state, station):\n",
    "    path = f'../data/weather/big_cities/slim/{station}.parquet'\n",
    "    if isfile(path):\n",
    "        return pd.read_parquet(path)\n",
    "\n",
    "    w = pd.read_parquet(download_file_path(state, station))\n",
    "\n",
    "    w = w.drop_duplicates('valid').set_index('valid')\n",
    "\n",
    "    print(f'Read {station}')\n",
    "    # Many stations provide more than one observation per hour, so subsample\n",
    "    w = w.head(10)\n",
    "    obs = observation_hours[1:100]\n",
    "    w = w.iloc[[w.index.get_loc(h, method='nearest') for h in obs]]\n",
    "\n",
    "    print(f'Hourly subsampled {station}')\n",
    "    # Each station's data will be merged into a single dataframe, so rename columns\n",
    "    w = w[['tmpf', 'feel']]\n",
    "    w = w.rename(columns={'tmpf':f\"{station}_tmpf\", 'feel':f\"{station}_feel\"})\n",
    "    #w = normalize_station(w)\n",
    "    return w\n",
    "\n",
    "#dfs = parallel(delayed(build_hourly_df)(state, station) for (state, station) in stations)\n",
    "dfs = []\n",
    "for (state, station) in stations:\n",
    "    dfs.append(build_hourly_df(state, station))\n",
    "\n",
    "merged = dfs[0]\n",
    "for w in dfs[1:]:\n",
    "    merged = merged.merge(w, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the Regional MTLF and Actual Load for each Observation Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching hourly Regional Forecasts and Actual Load for 2524 days\n"
     ]
    }
   ],
   "source": [
    "from rf_al_data import get_daily_rf_al_df\n",
    "forecast_output_dir = '../data/mtlf'\n",
    "rf_al_df = get_daily_rf_al_df(first_hour, last_hour, forecast_output_dir)\n",
    "\n",
    "rf_al_df.to_parquet(f'{forecast_output_dir}/rf_al_all.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m (num_weather_observations, _) \u001b[38;5;241m=\u001b[39m merged\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      2\u001b[0m (num_load_observations, _) \u001b[38;5;241m=\u001b[39m rf_al_df\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m num_weather_observations \u001b[38;5;241m==\u001b[39m num_load_observations\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(num_weather_observations, _) = merged.shape\n",
    "(num_load_observations, _) = rf_al_df.shape\n",
    "assert num_weather_observations == num_load_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60576"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_load_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58374"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_weather_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e76a2b364cc0264c10adfbdfe3b5ba02e67c79fb140c1f3503657a619a8cf93d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
