{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Data\n",
    "\n",
    "In this notebook we perform the following steps:\n",
    "* Establish the first hour of the dataset\n",
    "* For the first month,\n",
    "  * Obtain a list of available stations by state\n",
    "  * Obtain temperature observations from weather stations in the MISO footprint\n",
    "    * Stations are organized into MISO regions by state boundaries\n",
    "    * Stations are predominantly clustered in population centers, making many observation redundant\n",
    "    * There are many lacunae in some stations\n",
    "  * Obtain the actual hourly MISO Load data and historical Medium-Term Load Forecasts (MTLF)\n",
    "  * Join Load and MTLF data with weather observations to complete the raw data\n",
    "  * Persist the data and demonstrate use of dataset wrapper class\n",
    "* Update the dataset to the current day\n",
    "* Identify and mitigate lacunae\n",
    "* Publish the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Date Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from MISO_data import prevailing_time as est\n",
    "\n",
    "# beginning of MISO's historical records that include the southern region (zones 8-10)\n",
    "first_hour = est(2015, 2, 1, 0)\n",
    "# five hours weather data was lost for 2015-06-17\n",
    "# first_hour = est(2015, 7, 1, 0)\n",
    "\n",
    "# latest date with actual load data available is\n",
    "# l = date.today() - timedelta(days=2)\n",
    "# last_hour = est(l.year, l.month, l.day, 23)\n",
    "# instead, fix the date for repeatbility\n",
    "last_hour = est(2022, 3, 31, 23)\n",
    "\n",
    "test_split = last_hour - timedelta(days=364, hours=23)\n",
    "validation_split = test_split - timedelta(days=365)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Weather Data for Zones\n",
    "\n",
    "Source: [Iowa State ASOS Network Downloads](https://mesonet.agron.iastate.edu/request/download.phtml)\n",
    "\n",
    "`zones` below represents an initial candidate set of weather stations in each\n",
    "zone. We will examine the data quality from each of the stations in the next\n",
    "section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weather_data import ASOS\n",
    "from pathlib import Path\n",
    "from os.path import isfile\n",
    "import pandas as pd\n",
    "\n",
    "zones = { 1 : {'MN': ['MSP',  # Minneapolis / St. Paul (STP)\n",
    "                      'RST',  # Rochester\n",
    "                      'DYT'], # Duluth\n",
    "               'ND': ['FAR',  # Fargo\n",
    "                      'BIS',  # Bismarck\n",
    "                      'GFK'], # Grand Forks\n",
    "               'SD': ['ABR'], # Aberdeen\n",
    "               'WI': ['LSE'], # La Crosse\n",
    "               'IL': ['SFY']  # Savanna \n",
    "              },\n",
    "          2 : {'WI': ['MSN', 'MKE', 'EAU', 'GRB'],\n",
    "               'MI': ['ANJ', 'SAW', 'IWD']},\n",
    "          3 : {'IA': ['DSM', 'CID', 'DVN', 'SUX', 'ALO', 'MCW']}\n",
    "        }\n",
    "\n",
    "def download_file_path(zone, state, station):\n",
    "    zone_data = f\"./data/zone_{zone}\"\n",
    "    Path(zone_data).mkdir(exist_ok=True)\n",
    "    return f'{zone_data}/{state}_{station}.parquet'\n",
    "\n",
    "asos = ASOS()\n",
    "def download_station(zone, state, station):\n",
    "    path = download_file_path(zone, state, station) \n",
    "    if isfile(path):\n",
    "        return pd.read_parquet(path)\n",
    "\n",
    "    station = asos.get_hourly_observations(station, first_hour, last_hour)\n",
    "    if station is None:\n",
    "        print(f'Retrieve {station} failed')\n",
    "        return None\n",
    "    station.to_parquet(path)\n",
    "    return station\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "def do_parallel(func, zones):\n",
    "    parallel = Parallel(n_jobs=cpu_count())\n",
    "    result = {}\n",
    "    for zone in zones:\n",
    "        stations = [(state, station) for state in zones[zone].keys() for station in zones[zone][state]]\n",
    "        result[zone] = pd.concat(parallel(delayed(func)(zone, state, station) for (state, station) in stations))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_results = do_parallel(download_station, zones)\n",
    "all_zones = pd.concat(zone_results.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data Quality\n",
    "\n",
    "We will look for large lacunae in the original data by examining where a few hours or more were interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station\n",
       "ABR     20\n",
       "ALO     10\n",
       "ANJ     58\n",
       "BIS     12\n",
       "CID     30\n",
       "DSM      1\n",
       "DVN     32\n",
       "DYT     66\n",
       "EAU     15\n",
       "FAR     10\n",
       "GFK     20\n",
       "GRB     11\n",
       "IWD    126\n",
       "LSE     34\n",
       "MCW     51\n",
       "MKE      6\n",
       "MSN     11\n",
       "MSP      4\n",
       "RST     12\n",
       "SAW     11\n",
       "SFY    123\n",
       "SUX     50\n",
       "Name: temp, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated = all_zones[pd.isna(all_zones['observation_time'])].copy()\n",
    "f = interpolated.groupby([interpolated.index.date, 'station']).temp.count()\n",
    "f[f > 4].groupby(level='station').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station\n",
       "ABR    18\n",
       "ALO    23\n",
       "ANJ    24\n",
       "BIS    11\n",
       "CID    23\n",
       "DSM     6\n",
       "DVN    24\n",
       "DYT    24\n",
       "EAU    24\n",
       "FAR    16\n",
       "GFK    11\n",
       "GRB    11\n",
       "IWD    24\n",
       "LSE    24\n",
       "MCW    24\n",
       "MKE     9\n",
       "MSN     8\n",
       "MSP    11\n",
       "RST    21\n",
       "SAW     8\n",
       "SFY    24\n",
       "SUX    24\n",
       "Name: temp, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[f > 4].groupby(level='station').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Station for Each Zone\n",
    "\n",
    "Based on the above results, we select a single station in each zone with the fewest lacunae in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1_station = 'MSP'\n",
    "zone2_station = 'MKE'\n",
    "zone3_station = 'DSM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering: Temperature Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?data=tmpf&data=feel&tz=Etc/UTC&format=comma&latlon=yes&year1=2015&month1=1&day1=25&year2=2022&month2=4&day2=9&station=MSP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>station</th>\n",
       "      <th>MSP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-02-01 00:00:00-05:00</th>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 01:00:00-05:00</th>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 02:00:00-05:00</th>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 03:00:00-05:00</th>\n",
       "      <td>19.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01 04:00:00-05:00</th>\n",
       "      <td>19.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "station                      MSP\n",
       "hour                            \n",
       "2015-02-01 00:00:00-05:00  23.00\n",
       "2015-02-01 01:00:00-05:00  23.00\n",
       "2015-02-01 02:00:00-05:00  21.02\n",
       "2015-02-01 03:00:00-05:00  19.04\n",
       "2015-02-01 04:00:00-05:00  19.04"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = asos.get_hourly_observations(zone1_station, first_hour, last_hour)\n",
    "df1 = df.pivot(columns='station', values='temp')\n",
    "df2 = df1.shift(periods=1)\n",
    "df2.iloc[0] = df2.iloc[1] #replace NaN\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.000000     17223\n",
       "-0.900000      3470\n",
       "-2.000000      3387\n",
       "-1.100000      2848\n",
       " 2.000000      2737\n",
       "              ...  \n",
       "-11.880000        1\n",
       "-6.840000         1\n",
       "-15.840000        1\n",
       "-0.369590         1\n",
       " 2.022754         1\n",
       "Length: 549, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone1 = (df1 - df2)\n",
    "zone1.mean(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the Regional MTLF and Actual Load for each Observation Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rf_al_data import \n",
    "\n",
    "Path(\"./data/mtlf\").mkdir(exist_ok=True)\n",
    "forecast_output_dir = './data/mtlf'\n",
    "# the actuals aren't available until the next day\n",
    "actuals = get_daily_rf_al_df(first_hour, last_hour + timedelta(days=2.0), forecast_output_dir)\n",
    "actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonize Features with Actuals\n",
    "\n",
    "There are a number of lacunae in the weather observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mktime_idx(row): \n",
    "    return datetime.combine(row['Market Day'].date(), time(row['HourEnding'] - 1), timezone(timedelta(hours = -5)))\n",
    "\n",
    "actuals['time_idx'] = actuals.apply(mktime_idx, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals.to_parquet('./data/actuals_mtlf.parquet')\n",
    "actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df.pivot(index='valid', columns='station', values='tmpf').dropna()\n",
    "data = p.join(actuals.set_index('time_idx'), how='inner')\n",
    "\n",
    "(n, _) = data.shape\n",
    "(num_weather_observations, _) = df.groupby('valid').count().shape\n",
    "(num_load_observations, _) = actuals.shape\n",
    "(num_weather_observations, num_load_observations, len(observation_hours), n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Business Hours\n",
    "\n",
    "Can we improve the performance of the model by introducing business hours into the feature set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay, BusinessHour\n",
    "\n",
    "federal_business_days = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "bh = BusinessHour()\n",
    "def is_biz_hour(d):\n",
    "    return federal_business_days.is_on_offset(d) and bh.is_on_offset(d)\n",
    "data['IsBusinessHour'] = data.index.to_series().apply(lambda d: 1 if is_biz_hour(d) else 0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e76a2b364cc0264c10adfbdfe3b5ba02e67c79fb140c1f3503657a619a8cf93d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
